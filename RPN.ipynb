{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RPN.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyM6Fp3UBURfJ8M56YvHoOBo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"3QCILgjXGPLJ","colab_type":"code","colab":{}},"source":["#Imports\n","import numpy as np\n","import os\n","import glob\n","import cv2\n","!pip install xmltodict\n","import xmltodict\n","import tensorflow as tf\n","import math\n","from tqdm import tqdm\n","from PIL import Image, ImageDraw\n","from google.colab.patches import cv2_imshow"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QppSjIWIKhCh","colab_type":"code","colab":{}},"source":["\n","#download VOC data and extract .tar file\n","!mkdir train\n","!mkdir test\n","!wget http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar -P train/\n","!wget https://storage.googleapis.com/coco-dataset/external/PASCAL_VOC.zip -P train/\n","!wget http://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar -P test/\n","!tar -xf test/VOCtest_06-Nov-2007.tar -C test/\n","!tar -xf train/VOCtrainval_06-Nov-2007.tar -C train/\n","!unzip train/PASCAL_VOC.zip -d train/\n","!rm -rf train/PASCAL_VOC.zip train/VOCtrainval_06-Nov-2007.tar\n","!rm -rf test/VOCtest_06-Nov-2007.tar"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_JIp9A0LHAwt","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fzYVT9h2Gjo1","colab_type":"code","colab":{}},"source":["data_images_path     = os.getcwd()+'/train/VOCdevkit/VOC2007/JPEGImages'\n","data_annotation_path = os.getcwd()+'/train/VOCdevkit/VOC2007/Annotations'\n","image_height = 224\n","image_width  = 224\n","image_depth  = 3        # RGB\n","rpn_kernel_size = 3     # 3x3\n","subsampled_ratio = 8    # Pooling 3 times\n","anchor_sizes = [32,64,128]      # Using [128, 256, 512] sizes in 1000x600 img size / Used [32, 64, 128] sizes in 224x224\n","anchor_aspect_ratio = [[1,1],[1/math.sqrt(2),math.sqrt(2)],[math.sqrt(2),1/math.sqrt(2)]]\n","num_anchors_in_box = len(anchor_sizes)*len(anchor_aspect_ratio)\n","neg_threshold = 0.3\n","pos_threshold = 0.7\n","anchor_sampling_amount = 128        # 128 for each positive, negative sampling"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zy2n8V-HG2CN","colab_type":"code","colab":{}},"source":["list_images      = sorted([x for x in glob.glob(data_images_path + '/**')])    \n","#list_images=list_images[:2000]\n","total_images = len(list_images)\n","print(total_images)\n","list_annotations = sorted([x for x in glob.glob(data_annotation_path + '/**')]) \n","#list_annotations = list_annotations[:2000]\n","\n","# evaluating data consistency between images and annotations\n","t1=[]\n","t2=[]\n","for i in range(len(list_images)) :\n","    t1.append(list_images[i][-11:-4])\n","for i in range(len(list_annotations)) :\n","    t2.append(list_annotations[i][-11:-4])\n","\n","for i in range(len(list_annotations)) :\n","    if t2[i] not in t1 :\n","        print(list_annotations[i])\n","\n","print(len(list_annotations))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sXvffpMVG3OC","colab_type":"code","colab":{}},"source":["def get_classes(xml_files=list_annotations):\n","    '''\n","    Input : dataset's annotations\n","    parsing xml data to get objects label from images\n","    Output : class label\n","    \n","    '''\n","    classes = []\n","    \n","    for file in xml_files: \n","\n","        f = open(file)\n","        doc = xmltodict.parse(f.read()) #parse the xml file to python dict.\n","        # annotation - object - [obj1, obj2 , ...]\n","        try: \n","            \n","            for obj in doc['annotation']['object']:\n","                classes.append(obj['name'].lower()) \n","        # annotation - object\n","        except TypeError as e: \n","            classes.append(doc['annotation']['object']['name'].lower()) \n","\n","        f.close()\n","\n","    classes = list(set(classes)) \n","    classes.sort()\n","\n","    return classes"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0W2F4O8XG5ms","colab_type":"code","colab":{}},"source":["classes = get_classes(list_annotations)\n","print(classes)\n","num_of_class = len(classes)\n","print(num_of_class)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UDwvH-y8G69Y","colab_type":"code","colab":{}},"source":["def get_labels_from_xml(xml_file_path, num_of_class = num_of_class):\n","    '''\n","    Input : 1 xml file\n","    Get class label, gt box coordinates.\n","    Because images are resized to 224x224, coordinates also need to be resized.\n","    Output: Existing class label, ground truth box coordinates\n","    '''\n","\n","    f = open(xml_file_path)\n","    doc = xmltodict.parse(f.read()) \n","\n","\n","    ori_img_height = float(doc['annotation']['size']['height'])\n","    ori_img_width  = float(doc['annotation']['size']['width'])\n","\n","\n","    class_label = [] \n","    bbox_label  = [] \n","\n","    # multi-objects in image\n","    try:\n","        for each_obj in doc['annotation']['object']:\n","            obj_class = each_obj['name'].lower() \n","            # Get bounding box coordinates\n","            x_min = float(each_obj['bndbox']['xmin']) # top left x-axis coordinate.\n","            x_max = float(each_obj['bndbox']['xmax']) # bottom right x-axis coordinate.\n","            y_min = float(each_obj['bndbox']['ymin']) # top left y-axis coordinate.\n","            y_max = float(each_obj['bndbox']['ymax']) # bottom right y-axis coordinate.\n","\n","            # Images resized to 224x224. So resize the coordinates\n","            x_min = float((image_width/ori_img_width)*x_min)\n","            y_min = float((image_height/ori_img_height)*y_min)\n","            x_max = float((image_width/ori_img_width)*x_max)\n","            y_max = float((image_height/ori_img_height)*y_max)\n","\n","            generated_box_info = [x_min, y_min, x_max, y_max]       # [top-left, bottom-right]\n","\n","            index = classes.index(obj_class) \n","\n","            class_label.append(index)\n","            bbox_label.append(np.asarray(generated_box_info, dtype='float32'))\n","\n","    # single-object in image\n","    except TypeError as e : \n","\n","        obj_class = doc['annotation']['object']['name']\n","        x_min = float(doc['annotation']['object']['bndbox']['xmin']) \n","        x_max = float(doc['annotation']['object']['bndbox']['xmax']) \n","        y_min = float(doc['annotation']['object']['bndbox']['ymin']) \n","        y_max = float(doc['annotation']['object']['bndbox']['ymax']) \n","\n","        x_min = float((image_width/ori_img_width)*x_min)\n","        y_min = float((image_height/ori_img_height)*y_min)\n","        x_max = float((image_width/ori_img_width)*x_max)\n","        y_max = float((image_height/ori_img_height)*y_max)\n","\n","        generated_box_info = [x_min, y_min, x_max, y_max]\n","\n","        index = classes.index(obj_class) \n","\n","\n","        class_label.append(index)\n","        bbox_label.append(np.asarray(generated_box_info, dtype='float32'))\n","\n","\n","    return class_label, np.asarray(bbox_label)\n","xml_file_path=list_annotations[3]\n","\n","im = cv2.imread(list_images[3])\n","cv2_imshow(im)\n","\n","class_label, bbox_label = get_labels_from_xml(xml_file_path, num_of_class = num_of_class)\n","print(class_label)\n","print(bbox_label)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qvoLY11aG8wC","colab_type":"code","colab":{}},"source":["def generate_anchors(rpn_kernel_size=rpn_kernel_size, subsampled_ratio=subsampled_ratio,\n","                     anchor_sizes=anchor_sizes, anchor_aspect_ratio=anchor_aspect_ratio):\n","\n","    '''\n","    Input : subsample_ratio (=Pooled ratio)\n","    generate anchor in feature map. Then project it to original image.\n","    Output : list of anchors (x,y,w,h) and anchor_boolean (ignore anchor if value equals 0)\n","\n","    '''\n","\n","    list_of_anchors = []\n","    anchor_booleans = [] #This is to keep track of an anchor's status. Anchors that are out of boundary are meant to be ignored.\n","\n","    starting_center = divmod(rpn_kernel_size, 2)[0] # rpn kernel's starting center in feature map\n","    \n","    anchor_center = [starting_center - 1,starting_center] # -1 on the x-coor because the increment comes first in the while loop\n","    \n","    subsampled_height = image_height/subsampled_ratio       # = 28\n","    subsampled_width = image_width/subsampled_ratio         # = 28\n","    \n","    while (anchor_center != [subsampled_width - (1 + starting_center), subsampled_height - (1 + starting_center)]):  # != [26, 26]\n","\n","        anchor_center[0] += 1 #Increment x-axis\n","\n","        #If sliding window reached last center, increase y-axis\n","        if anchor_center[0] > subsampled_width - (1 + starting_center):\n","            anchor_center[1] += 1\n","            anchor_center[0] = starting_center\n","\n","        #anchors are referenced to the original image. \n","        #Therefore, multiply downsampling ratio to obtain input image's center \n","        anchor_center_on_image = [anchor_center[0]*subsampled_ratio, anchor_center[1]*subsampled_ratio]\n","\n","        for size in anchor_sizes:\n","            \n","            for a_ratio in anchor_aspect_ratio:\n","                # [x,y,w,h]\n","                anchor_info = [anchor_center_on_image[0], anchor_center_on_image[1], size*a_ratio[0], size*a_ratio[1]]\n","\n","                # check whether anchor crosses the boundary of the image or not\n","                if (anchor_info[0] - anchor_info[2]/2 < 0 or anchor_info[0] + anchor_info[2]/2 > image_width or \n","                                        anchor_info[1] - anchor_info[3]/2 < 0 or anchor_info[1] + anchor_info[3]/2 > image_height) :\n","\n","                    anchor_booleans.append([0.0])       # if anchor crosses boundary, anchor_booleans=0\n","\n","                else:\n","\n","                    anchor_booleans.append([1.0])\n","\n","                list_of_anchors.append(anchor_info)\n","    \n","    return list_of_anchors, anchor_booleans\n","\n","generate_anchors()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"joStnOmpHA1c","colab_type":"code","colab":{}},"source":["def generate_label(class_labels, ground_truth_boxes, anchors, anchor_booleans, num_class=num_of_class,\n","                    neg_anchor_thresh = neg_threshold, pos_anchor_thresh = pos_threshold):\n","    '''\n","    Input  : classes, ground truth box (top-left, bottom-right), all of anchors, anchor booleans.\n","    Compute IoU to get positive, negative samples.\n","    if IoU > 0.7, positive / IoU < 0.3, negative / Otherwise, ignore\n","    Output : anchor booleans (to know which anchor to ignore), objectness label, regression coordinate in one image\n","    '''\n","\n","\n","    number_of_anchors = len(anchors) #Get the total number of anchors.\n","\n","    anchor_boolean_array   = np.reshape(np.asarray(anchor_booleans),(number_of_anchors, 1))\n","    \n","    # IoU is more than threshold or not.\n","    objectness_label_array = np.zeros((number_of_anchors, 2), dtype=np.float32)\n","    # delta(x, y, w, h)\n","    box_regression_array   = np.zeros((number_of_anchors, 4), dtype=np.float32)\n","    # belongs to which object for every anchor\n","    class_array            = np.zeros((number_of_anchors, num_class), dtype=np.float32)\n","    \n","    for j in range(ground_truth_boxes.shape[0]):\n","\n","        #Get the ground truth box's coordinates.\n","        gt_box_top_left_x = ground_truth_boxes[j][0]\n","        gt_box_top_left_y = ground_truth_boxes[j][1]\n","        gt_box_btm_rght_x = ground_truth_boxes[j][2]\n","        gt_box_btm_rght_y = ground_truth_boxes[j][3]\n","\n","        #Calculate the area of the original bounding box.1 is added since the index starts from 0 not 1.\n","        gt_box_area = (gt_box_btm_rght_x - gt_box_top_left_x + 1)*(gt_box_btm_rght_y - gt_box_top_left_y + 1)\n","\n","    \n","        for i in range(number_of_anchors):\n","\n","            ######### Compute IoU #########\n","\n","            # Check if the anchor should be ignored or not. If it is to be ignored, it crosses boundary of image.\n","            if int(anchor_boolean_array[i][0]) == 0:\n","\n","                continue\n","\n","            anchor = anchors[i] #Select the i-th anchor [x,y,w,h]\n","\n","            #anchors are in [x,y,w,h] format, convert them to the [top-left-x, top-left-y, btm-right-x, btm-right-y]\n","            anchor_top_left_x = anchor[0] - anchor[2]/2\n","            anchor_top_left_y = anchor[1] - anchor[3]/2\n","            anchor_btm_rght_x = anchor[0] + anchor[2]/2\n","            anchor_btm_rght_y = anchor[1] + anchor[3]/2\n","\n","            # Get the area of the bounding box.\n","            anchor_box_area = (anchor_btm_rght_x - anchor_top_left_x + 1)*(anchor_btm_rght_y - anchor_top_left_y + 1)\n","\n","            # Determine the intersection rectangle.\n","            int_rect_top_left_x = max(gt_box_top_left_x, anchor_top_left_x)\n","            int_rect_top_left_y = max(gt_box_top_left_y, anchor_top_left_y)\n","            int_rect_btm_rght_x = min(gt_box_btm_rght_x, anchor_btm_rght_x)\n","            int_rect_btm_rght_y = min(gt_box_btm_rght_y, anchor_btm_rght_y)\n","\n","            # if the boxes do not intersect, difference = 0\n","            int_rect_area = max(0, int_rect_btm_rght_x - int_rect_top_left_x + 1)*max(0, int_rect_btm_rght_y - int_rect_top_left_y)\n","\n","            # Calculate the IoU\n","            intersect_over_union = float(int_rect_area / (gt_box_area + anchor_box_area - int_rect_area))\n","            \n","            # Positive\n","            if intersect_over_union >= pos_anchor_thresh:\n","\n","                objectness_label_array[i][0] = 1.0 \n","                objectness_label_array[i][1] = 0.0 \n","                \n","                #get the class label\n","                class_label = class_labels[j]\n","                class_array[i][int(class_label)] = 1.0 #Denote the label of the class in the array.\n","                \n","                #Get the ground-truth box's [x,y,w,h]\n","                gt_box_center_x = ground_truth_boxes[j][0] + ground_truth_boxes[j][2]/2\n","                gt_box_center_y = ground_truth_boxes[j][1] + ground_truth_boxes[j][3]/2\n","                gt_box_width    = ground_truth_boxes[j][2] - ground_truth_boxes[j][0]\n","                gt_box_height   = ground_truth_boxes[j][3] - ground_truth_boxes[j][1]\n","\n","                #Regression loss / weight\n","                delta_x = (gt_box_center_x - anchor[0])/anchor[2]\n","                delta_y = (gt_box_center_y - anchor[1])/anchor[3]\n","                delta_w = math.log(gt_box_width/anchor[2])\n","                delta_h = math.log(gt_box_height/anchor[3])\n","\n","                box_regression_array[i][0] = delta_x\n","                box_regression_array[i][1] = delta_y\n","                box_regression_array[i][2] = delta_w\n","                box_regression_array[i][3] = delta_h\n","\n","            if intersect_over_union <= neg_anchor_thresh:\n","                if int(objectness_label_array[i][0]) == 0:\n","                    objectness_label_array[i][1] = 1.0\n","\n","            if intersect_over_union > neg_anchor_thresh and intersect_over_union < pos_anchor_thresh:\n","                if int(objectness_label_array[i][0]) == 0 and int(objectness_label_array[i][1]) == 0:\n","                    anchor_boolean_array[i][0] = 0.0 # ignore this anchor\n","\n","\n","    return anchor_boolean_array, objectness_label_array, box_regression_array, class_array"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m0IThavQHEfv","colab_type":"code","colab":{}},"source":["def anchor_sampling(anchor_booleans, objectness_label, anchor_sampling_amount=anchor_sampling_amount):\n","\n","    '''\n","    Input : anchor booleans and objectness label\n","    fixed amount of negative anchors and positive anchors for training. \n","    If we use all the neg and pos anchors, model will overfit on the negative samples.\n","    Output: Updated anchor booleans. \n","    '''\n","\n","    positive_count = 0\n","    negative_count = 0\n","    \n","    for i in range(objectness_label.shape[0]):\n","        if int(objectness_label[i][0]) == 1: #If the anchor is positive\n","\n","            if positive_count > anchor_sampling_amount: #If the positive anchors are more than the threshold amount, set the anchor boolean to 0.\n","\n","                anchor_booleans[i][0] = 0.0\n","\n","            positive_count += 1\n","\n","        if int(objectness_label[i][1]) == 1: #If the anchor is negatively labelled.\n","            if negative_count > anchor_sampling_amount: #If the negative anchors are more than the threshold amount, set the boolean to 0.\n","\n","                anchor_booleans[i][0] = 0.0\n","\n","            negative_count += 1\n","    \n","    return anchor_booleans\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nvyt69uEHHU1","colab_type":"code","colab":{}},"source":["def generate_dataset(first_index, last_index, anchors, anchor_booleans):\n","        '''\n","        Input : starting index and final index of the dataset to be generated.\n","        Output: Anchor booleans, Objectness Label and Regression Label in batches.\n","        '''\n","        num_of_anchors = len(anchors)\n","        \n","        batch_anchor_booleans   = []\n","        batch_objectness_array  = []\n","        batch_regression_array  = []\n","        batch_class_label_array = []\n","\n","        for i in range(first_index, last_index):\n","\n","            #Get the true labels and the ground truth boxes [x,y,w,h] for every file.\n","            true_labels, ground_truth_boxes = get_labels_from_xml(xml_file_path=list_annotations[i])\n","\n","            # generate_labels for specified batches\n","            anchor_bools, objectness_label_array, box_regression_array, class_array = generate_label(true_labels, ground_truth_boxes, \n","                                                                                                        anchors, anchor_booleans)\n","            #ggenerate_label(class_labels, ground_truth_boxes, anchors, anchor_booleans, num_class=num_of_class,\n","            #        neg_anchor_thresh = neg_threshold, pos_anchor_thresh = pos_threshold)\n","\n","            # get the updated anchor bools based on the fixed number of sample\n","            anchor_bools = anchor_sampling(anchor_bools, objectness_label_array)\n","            \n","            batch_anchor_booleans.append(anchor_bools)\n","            batch_objectness_array.append(objectness_label_array)\n","            batch_regression_array.append(box_regression_array)\n","            batch_class_label_array.append(class_array)\n","\n","        batch_anchor_booleans   = np.reshape(np.asarray(batch_anchor_booleans), (-1,num_of_anchors))            # (1, 6084, 1) -> (1, 6084)\n","        \n","        batch_objectness_array  = np.asarray(batch_objectness_array)\n","        batch_regression_array  = np.asarray(batch_regression_array)\n","        batch_class_label_array = np.asarray(batch_class_label_array)\n","\n","        return (batch_anchor_booleans, batch_objectness_array, batch_regression_array, batch_class_label_array)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wqQCGHU5HJEe","colab_type":"code","colab":{}},"source":["def read_images(first_index, last_index):\n","    '''\n","    Read the image files, then resize.\n","    Input : first and last index.\n","    Output: numpy array of images.\n","    '''\n","    images_list = []\n","    \n","    for i in range(first_index, last_index):\n","        \n","        im = cv2.imread(list_images[i])\n","        im = cv2.resize(im, (image_height, image_width))/255\n","        \n","        images_list.append(im)\n","    \n","    return np.asarray(images_list)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"isM7-kOFHKym","colab_type":"code","colab":{}},"source":["anchors, an_bools = generate_anchors() #We only need to generate the anchors and the anchor booleans once.\n","num_of_anchors = len(anchors)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4bo78X7oHM1C","colab_type":"code","colab":{}},"source":["a,b,c,d = generate_dataset(0,1, anchors, an_bools)\n","a.shape\n","print(a)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P9X2aZphHN18","colab_type":"code","colab":{}},"source":["learning_rate = 1e-5\n","epoch = 100\n","batch_size = 10\n","!mkdir -p '/content/drive/My Drive/VOCdata/trained_weight'\n","model_checkpoint = './drive/My Drive/VOCdata/trained_weight/model.ckpt'\n","decay_steps = 10000\n","decay_rate = 0.99\n","lambda_value = 10"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"acSqLwUD-kLx","colab_type":"code","colab":{}},"source":["def smooth_func(t):\n","    \n","    t = tf.abs(t)\n","    \n","    comparison_tensor = tf.ones((num_of_anchors, 4))\n","    smoothed = tf.where(tf.less(t, comparison_tensor), 0.5*tf.pow(t,2), t - 0.5)\n","    \n","    return smoothed"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AlqpqYl8-ltY","colab_type":"code","colab":{}},"source":["\n","def smooth_L1(pred_box, truth_box):\n","    \n","    diff = pred_box - truth_box\n","    \n","    smoothed = tf.map_fn(smooth_func, diff)\n","    \n","    return smoothed"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eDEFMKcd-m9-","colab_type":"code","colab":{}},"source":["X       = tf.placeholder(tf.float32, shape=(None, image_height, image_width, image_depth)) \n","Y_obj   = tf.placeholder(tf.float32, shape=(None, num_of_anchors,2))\n","Y_coor  = tf.placeholder(tf.float32, shape=(None, num_of_anchors,4))\n","anch_bool = tf.placeholder(tf.float32, shape=(None, num_of_anchors))\n","\n","conv1 = tf.contrib.layers.conv2d(X, num_outputs=64, kernel_size=3, stride=1, \n","                                 padding='SAME', activation_fn=tf.nn.relu)\n","conv2 = tf.contrib.layers.conv2d(conv1, num_outputs=64, kernel_size=3, stride=1, \n","                                 padding='SAME', activation_fn=tf.nn.relu)\n","conv2_pool = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n","\n","\n","conv3 = tf.contrib.layers.conv2d(conv2_pool, num_outputs=128, kernel_size=3, stride=1, \n","                                 padding='SAME', activation_fn=tf.nn.relu)\n","conv4 = tf.contrib.layers.conv2d(conv3, num_outputs=128, kernel_size=3, stride=1, \n","                                 padding='SAME', activation_fn=tf.nn.relu)\n","conv4_pool = tf.nn.max_pool(conv4, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n","\n","conv5 = tf.contrib.layers.conv2d(conv4_pool, num_outputs=256, kernel_size=3, stride=1, \n","                                 padding='SAME', activation_fn=tf.nn.relu)\n","conv6 = tf.contrib.layers.conv2d(conv5, num_outputs=256, kernel_size=3, stride=1, \n","                                 padding='SAME', activation_fn=tf.nn.relu)\n","conv7 = tf.contrib.layers.conv2d(conv6, num_outputs=256, kernel_size=3, stride=1, \n","                                 padding='SAME', activation_fn=tf.nn.relu)\n","conv7_pool = tf.nn.max_pool(conv7, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')          # 28x28x256"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OrHWV35p2GGN","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"m2JYUXi3-p0a","colab_type":"code","colab":{}},"source":["rpn_conv = tf.contrib.layers.conv2d(conv7_pool, num_outputs=512, kernel_size=3, stride=1,           # 26x26x512\n","                                    padding='VALID', activation_fn=tf.nn.relu)\n","\n","obj_conv = tf.contrib.layers.conv2d(rpn_conv, num_outputs=18, kernel_size=1, stride=1, padding='VALID', activation_fn=None)         # 26x26x18\n","bb_conv = tf.contrib.layers.conv2d(rpn_conv, num_outputs=36, kernel_size=1, stride=1, padding='VALID', activation_fn=None)          # 26x26x36\n","\n","class_conv_reshape = tf.reshape(obj_conv, (-1, num_of_anchors, 2))          # 6084x2\n","anchor_conv_reshape = tf.reshape(bb_conv, (-1, num_of_anchors, 4))          # 6084x4\n","\n","logits = tf.nn.softmax(class_conv_reshape)\n","\n","global_step = tf.Variable(0, trainable=False)\n","decayed_lr = tf.train.exponential_decay(learning_rate,\n","                                            global_step, decay_steps,\n","                                            decay_rate, staircase=True)\n","\n","\n","\n","loss1 = 1/256*tf.reduce_sum(anch_bool*(tf.nn.softmax_cross_entropy_with_logits(labels=Y_obj, logits=class_conv_reshape)))       # positive(128) + negative(128)\n","# (10, 6084, 2)\n","loss2 = lambda_value*(1/128)*tf.reduce_sum((tf.reshape(Y_obj[:,:,0], (-1,num_of_anchors,1)))*smooth_L1(anchor_conv_reshape, Y_coor))\n","\n","total_loss = loss1 + loss2\n","\n","optimizer = tf.train.AdamOptimizer(decayed_lr).minimize(total_loss, global_step=global_step)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M8GmIAcv-wKo","colab_type":"code","colab":{}},"source":["sess = tf.InteractiveSession()\n","sess.run(tf.global_variables_initializer())\n","saver = tf.train.Saver()\n","\n","try:\n","    saver.restore(sess, model_checkpoint)\n","    print(\"Model has been loaded!\")\n","    \n","except:\n","    \n","    print(\"Model doens't exist!\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MI3fc5DQ-ylY","colab_type":"code","colab":{}},"source":["\n","def draw_a_rectangel_in_img(draw_obj, box, color, width):\n","    '''\n","    use draw lines to draw rectangle. since the draw_rectangle func can not modify the width of rectangle\n","    :param draw_obj:\n","    :param box: [x1, y1, x2, y2]\n","    :return:\n","    '''\n","    x1, y1, x2, y2 = box[0], box[1], box[2], box[3]\n","    top_left, top_right = (x1, y1), (x2, y1)\n","    bottom_left, bottom_right = (x1, y2), (x2, y2)\n","\n","    draw_obj.line(xy=[top_left, top_right],\n","                  fill=color,\n","                  width=width)\n","    draw_obj.line(xy=[top_left, bottom_left],\n","                  fill=color,\n","                  width=width)\n","    draw_obj.line(xy=[bottom_left, bottom_right],\n","                  fill=color,\n","                  width=width)\n","    draw_obj.line(xy=[top_right, bottom_right],\n","                  fill=color,\n","                  width=width)\n","PIXEL_MEAN = [123.68, 116.779, 103.939]\n","def draw_boxes_with_label_and_scores(img_array, boxes):\n","\n","    img_array = img_array + np.array(PIXEL_MEAN)\n","    img_array.astype(np.float32)\n","    boxes = boxes.astype(np.int64)\n","    #labels = labels.astype(np.int32)\n","    img_array = np.array(img_array * 255 / np.max(img_array), dtype=np.uint8)\n","    \n","    img_obj = Image.fromarray(img_array)\n","    #img_obj=img_array\n","    print('zz')\n","    raw_img_obj = img_obj.copy()\n","    \n","    draw_obj = ImageDraw.Draw(img_obj)\n","    num_of_objs = 0\n","    for box in boxes:\n","         draw_a_rectangel_in_img(draw_obj, box, color='Coral', width=3)\n","        \n","\n","    out_img_obj = Image.blend(raw_img_obj, img_obj, alpha=0.6)\n","\n","    return np.array(out_img_obj)\n","\n","#TRAINING \n","\n","for epoch_idx in range(epoch): #Each epoch.\n","    \n","    #Loop through the whole dataset in batches.\n","    for start_idx in tqdm(range(0, total_images, batch_size)):\n","        \n","        end_idx = start_idx + batch_size\n","        \n","        if end_idx >= total_images : end_idx = total_images - 1 #In case the end index exceeded the dataset.\n","            \n","        images = read_images(start_idx, end_idx) #Read images.\n","        \n","        #Get the labels needed.\n","        batch_anchor_booleans, batch_objectness_array, batch_regression_array, _ = \\\n","                                                generate_dataset(start_idx,end_idx, anchors, an_bools)\n","        print(batch_objectness_array.shape)\n","        #Optimize the model.\n","        anchor_reshape, _, theloss = sess.run([anchor_conv_reshape, optimizer, total_loss], feed_dict={X: images,\n","                                                                  Y_obj:batch_objectness_array,\n","                                                                  Y_coor: batch_regression_array,\n","                                                                  anch_bool: batch_anchor_booleans})\n","        \n","        '''\n","        img_array = cv2.imread(list_images[0])\n","            #img_array = cv2.resize(img_array, (image_height, image_width))/255\n","        img_array = np.array(img_array, np.float32) - np.array(PIXEL_MEAN)\n","\n","        anchor_booleans, objectness_array, regression_array, _ = \\\n","                                                generate_dataset(0,1, anchors, an_bools)\n","        img_array_tensor=read_images(0, 1)\n","        anchor_reshape=sess.run([anchor_conv_reshape], feed_dict={X:img_array_tensor, Y_obj : objectness_array, Y_coor : regression_array, anch_bool : anchor_booleans})\n","        print(anchor_reshape)\n","        print(anchor_reshape[0])\n","        \n","        anchor_reshape=anchor_reshape[0]\n","        boxes = np.array(\n","        [[200, 200, 500, 500],\n","            [300, 300, 400, 400],\n","            [200, 200, 400, 400]]\n","    )\n","        anchor_reshape=np.array(anchor_reshape)\n","        \n","        im=draw_boxes_with_label_and_scores(img_array, np.array(anchor_reshape))\n","        cv2_imshow(im)        \n","        '''\n","    #Save the model periodically.\n","        saver.save(sess, model_checkpoint)\n","    \n","    print(\"Epoch : %d, Loss : %g\"%(epoch_idx, theloss))\n","    img_array = cv2.imread(list_images[0])\n","        #img_array = cv2.resize(img_array, (image_height, image_width))/255\n","    img_array = np.array(img_array, np.float32) - np.array(PIXEL_MEAN)\n","\n","    anchor_booleans, objectness_array, regression_array, _ = \\\n","                                            generate_dataset(0,1, anchors, an_bools)\n","    img_array_tensor=read_images(0, 1)\n","    anchor_reshape=sess.run([anchor_conv_reshape], feed_dict={X:img_array_tensor, Y_obj : objectness_array, Y_coor : regression_array, anch_bool : anchor_booleans})\n","    print(anchor_reshape)\n","    print(anchor_reshape[0])\n","    anchor_reshape=anchor_reshape[0][0]\n","    boxes = np.array(\n","    [[200, 200, 500, 500],\n","        [300, 300, 400, 400],\n","        [200, 200, 400, 400]]\n",")\n","    im=draw_boxes_with_label_and_scores(img_array, np.array(anchor_reshape))\n","    cv2_imshow(im)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PBbxs5h5bvKz","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}